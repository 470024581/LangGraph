"""
å›ç­”Agent
åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ç”Ÿæˆé«˜è´¨é‡çš„å›ç­”
"""
from typing import List, Dict, Any, Optional
import os
from datetime import datetime

from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, BaseMessage
from langchain_community.callbacks.manager import get_openai_callback
from loguru import logger

from ..schemas.agent_state import AgentState, ProcessingStatus
from ..utils.ollama_provider import OllamaProvider


class AnswerAgent:
    """å›ç­”ç”ŸæˆAgent"""
    
    def __init__(self, ollama_provider: OllamaProvider):
        """åˆå§‹åŒ–å›ç­”Agent"""
        self.llm = ollama_provider.get_llm()
        self.is_ollama = True # å‡è®¾è¿™ä¸ªAgentæ€»æ˜¯ä½¿ç”¨Ollama
        logger.info(f"ä½¿ç”¨ Ollama æ¨¡å‹: {ollama_provider.model_name}")
        
        self.max_context_length = 4000  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        
    async def generate_answer(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆå›ç­”çš„ä¸»å…¥å£å‡½æ•°"""
        try:
            state.processing_status = ProcessingStatus.PROCESSING
            state.current_step = "answer_generation"
            
            # æ„å»ºprompt
            prompt = self._build_prompt(state)
            
            # ç”Ÿæˆå›ç­”
            answer = await self._generate_response(prompt)
            
            # æå–ç­”æ¡ˆæ¥æº
            sources = self._extract_sources(state)
            
            state.generated_answer = answer
            state.answer_sources = sources
            
            state.processing_status = ProcessingStatus.COMPLETED
            state.current_step = "review"
            
            logger.info(f"å›ç­”ç”Ÿæˆå®Œæˆï¼Œå­—æ•°: {len(answer)}")
            return state
            
        except Exception as e:
            logger.error(f"å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
            state.processing_status = ProcessingStatus.FAILED
            state.error_message = f"å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
            return state
    
    def _build_prompt(self, state: AgentState) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        
        # ç³»ç»Ÿè§’è‰²å®šä¹‰
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„ç›¸å…³ä¿¡æ¯ï¼Œå‡†ç¡®ã€è¯¦ç»†ã€æœ‰ç”¨åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ç›¸å…³ä¿¡æ¯è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯
2. å¦‚æœä¿¡æ¯ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º
4. é€‚å½“å¼•ç”¨å…·ä½“çš„ä¿¡æ¯æ¥æº
5. ä½¿ç”¨ä¸­æ–‡å›ç­”
6. ä¿æŒä¸“ä¸šå’Œå®¢è§‚çš„è¯­è°ƒ

å¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠæ•°æ®åˆ†æï¼Œè¯·æä¾›å…·ä½“çš„æ•°æ®å’Œåˆ†æç»“æœã€‚
å¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠæ–‡æ¡£å†…å®¹ï¼Œè¯·å¼•ç”¨ç›¸å…³æ®µè½å¹¶è¯´æ˜å‡ºå¤„ã€‚"""

        # ç”¨æˆ·é—®é¢˜
        user_query = state.user_query
        
        # ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯
        context = state.relevant_context if state.relevant_context else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
        # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡
        if len(context) > self.max_context_length:
            context = context[:self.max_context_length] + "...\n(ä¸Šä¸‹æ–‡å·²æˆªæ–­)"
        
        # æ„å»ºå®Œæ•´prompt
        full_prompt = f"""{system_prompt}

ç›¸å…³ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜: {user_query}

è¯·åŸºäºä¸Šè¿°ç›¸å…³ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"""

        return full_prompt
    
    async def _generate_response(self, prompt: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆå›ç­”"""
        try:
            messages = [HumanMessage(content=prompt)]
            
            # ä½¿ç”¨ Ollama ç”Ÿæˆ
            response_text = await self.llm.ainvoke(prompt)
            if isinstance(response_text, BaseMessage):
                 response_text = response_text.content
            
            logger.info(f"Ollamaç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(response_text)}")
            return response_text
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            return f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›ç­”ã€‚é”™è¯¯ä¿¡æ¯: {e}"
    
    def _extract_sources(self, state: AgentState) -> List[str]:
        """æå–å›ç­”æ¥æº"""
        sources = []
        
        if state.retrieval_results and state.retrieval_results.chunks:
            for chunk in state.retrieval_results.chunks:
                source_info = f"{chunk.source_file}"
                
                if hasattr(chunk, 'page_number') and chunk.page_number:
                    source_info += f" (ç¬¬{chunk.page_number}é¡µ)"
                
                if "source_type" in chunk.metadata and chunk.metadata.get("source_type") == "database":
                    table_name = chunk.metadata.get("table_name", "æœªçŸ¥è¡¨")
                    source_info += f" - æ•°æ®è¡¨: {table_name}"
                
                if source_info not in sources:
                    sources.append(source_info)
        
        return sources
    
    async def regenerate_answer(self, state: AgentState, feedback: str = "") -> AgentState:
        """åŸºäºåé¦ˆé‡æ–°ç”Ÿæˆå›ç­”"""
        try:
            state.revision_count += 1
            state.processing_status = ProcessingStatus.PROCESSING
            state.current_step = "answer_regeneration"
            
            # æ„å»ºåŒ…å«åé¦ˆçš„æ”¹è¿›prompt
            improved_prompt = self._build_improved_prompt(state, feedback)
            
            # é‡æ–°ç”Ÿæˆå›ç­”
            new_answer = await self._generate_response(improved_prompt)
            
            state.generated_answer = new_answer
            state.processing_status = ProcessingStatus.COMPLETED
            state.current_step = "review"
            
            logger.info(f"å›ç­”é‡æ–°ç”Ÿæˆå®Œæˆ (ç¬¬{state.revision_count}æ¬¡ä¿®è®¢)")
            return state
            
        except Exception as e:
            logger.error(f"é‡æ–°ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
            state.processing_status = ProcessingStatus.FAILED
            state.error_message = f"é‡æ–°ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"
            return state
    
    def _build_improved_prompt(self, state: AgentState, feedback: str) -> str:
        """æ„å»ºåŒ…å«æ”¹è¿›å»ºè®®çš„prompt"""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ä½ éœ€è¦åŸºäºä¹‹å‰çš„å›ç­”å’Œå®¡é˜…åé¦ˆï¼Œç”Ÿæˆä¸€ä¸ªæ”¹è¿›çš„å›ç­”ã€‚

æ”¹è¿›è¦æ±‚ï¼š
1. ä»”ç»†è€ƒè™‘å®¡é˜…åé¦ˆä¸­çš„å»ºè®®
2. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
3. æ”¹è¿›å›ç­”çš„ç»“æ„å’Œè¡¨è¾¾
4. è¡¥å……é—æ¼çš„é‡è¦ä¿¡æ¯
5. ç¡®ä¿å›ç­”æ›´åŠ å…¨é¢å’Œæœ‰ç”¨
6. ä½¿ç”¨ä¸­æ–‡å›ç­”"""
        
        previous_answer = state.generated_answer
        context = state.relevant_context if state.relevant_context else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        user_query = state.user_query
        
        # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡
        if len(context) > self.max_context_length:
            context = context[:self.max_context_length] + "...\n(ä¸Šä¸‹æ–‡å·²æˆªæ–­)"
        
        improved_prompt = f"""{system_prompt}

åŸå§‹é—®é¢˜: {user_query}

ç›¸å…³ä¿¡æ¯:
{context}

ä¹‹å‰çš„å›ç­”:
{previous_answer}

å®¡é˜…åé¦ˆ:
{feedback}

è¯·åŸºäºå®¡é˜…åé¦ˆæ”¹è¿›ä½ çš„å›ç­”ï¼š"""

        return improved_prompt
    
    def get_answer_statistics(self, state: AgentState) -> Dict[str, Any]:
        """è·å–å›ç­”ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "answer_length": len(state.generated_answer) if state.generated_answer else 0,
            "revision_count": state.revision_count,
            "sources_count": len(state.answer_sources),
            "has_database_sources": any("æ•°æ®è¡¨" in source for source in state.answer_sources),
            "has_document_sources": any("æ•°æ®è¡¨" not in source for source in state.answer_sources)
        }
        
        if state.retrieval_results:
            stats["retrieved_chunks"] = len(state.retrieval_results.chunks)
        
        return stats
    
    def format_final_answer(self, state: AgentState) -> str:
        """æ ¼å¼åŒ–æœ€ç»ˆå›ç­”"""
        if not state.generated_answer:
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚"
        
        answer = state.generated_answer
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        if state.answer_sources:
            answer += "\n\nğŸ“š **ä¿¡æ¯æ¥æº:**\n"
            for i, source in enumerate(state.answer_sources, 1):
                answer += f"{i}. {source}\n"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats = self.get_answer_statistics(state)
        if stats["revision_count"] > 0:
            answer += f"\n*ï¼ˆæ­¤å›ç­”ç»è¿‡ {stats['revision_count']} æ¬¡ä¿®è®¢ä¼˜åŒ–ï¼‰*"
        
        return answer
    
    async def stream_answer(self, state: AgentState):
        """æµå¼ç”Ÿæˆå›ç­”ï¼ˆç”¨äºå®æ—¶æ˜¾ç¤ºï¼‰"""
        prompt = self._build_prompt(state)
        
        try:
            messages = [HumanMessage(content=prompt)]
            
            # Ollama æµå¼ç”Ÿæˆ
            async for chunk in self.llm.astream(messages):
                yield chunk
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}" 