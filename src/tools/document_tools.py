"""
æ–‡æ¡£å·¥å…·é“¾
æä¾›æ–‡æ¡£æ‘˜è¦ã€ç¿»è¯‘ã€åˆ†æç­‰å·¥å…·å‡½æ•°
"""
from typing import List, Dict, Any, Optional
import asyncio

from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, BaseMessage
from langchain_community.callbacks.manager import get_openai_callback
from loguru import logger

from ..schemas.agent_state import DocumentChunk, AgentState
from ..utils.ollama_provider import OllamaProvider
from ..components.document_processor import DocumentProcessor


class DocumentTools:
    """
    ä¸€ä¸ªé›†æˆäº†æ–‡æ¡£å¤„ç†ã€å‘é‡å­˜å‚¨å’Œä¿¡æ¯æ£€ç´¢çš„å·¥å…·é›†ã€‚
    """
    
    def __init__(self, ollama_provider: OllamaProvider):
        """åˆå§‹åŒ–æ–‡æ¡£å·¥å…·é›†"""
        self.document_processor = DocumentProcessor()
        # vector_store åœ¨ document_processor å†…éƒ¨åˆå§‹åŒ–ï¼Œåœ¨å…¶å¤„ç†æ–‡æ¡£åå¯ç”¨
        self.vector_store = None 
        
        self.llm = ollama_provider.get_llm()
        logger.info(f"æ–‡æ¡£å·¥å…·ä½¿ç”¨ Ollama æ¨¡å‹: {ollama_provider.model_name}")
    
    async def summarize_document(self, chunks: List[DocumentChunk], max_length: int = 500) -> str:
        """æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ"""
        if not chunks:
            return "æ— æ–‡æ¡£å†…å®¹å¯æ‘˜è¦"
        
        # åˆå¹¶æ–‡æ¡£å†…å®¹
        combined_content = "\n\n".join([chunk.content for chunk in chunks[:10]])  # é™åˆ¶å—æ•°é‡
        
        # æ„å»ºæ‘˜è¦prompt
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æ¡£å†…å®¹è¿›è¡Œæ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. æå–ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯
2. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œç»“æ„å®Œæ•´
3. æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…
4. ä½¿ç”¨ä¸­æ–‡

æ–‡æ¡£å†…å®¹ï¼š
{combined_content}

æ‘˜è¦ï¼š"""

        try:
            messages = [HumanMessage(content=prompt)]
            
            summary = await self.llm.generate(messages)
            
            logger.info(f"æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(summary)}")
            return summary
            
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def translate_text(self, text: str, target_language: str = "è‹±æ–‡") -> str:
        """æ–‡æœ¬ç¿»è¯‘"""
        if not text.strip():
            return "æ— å†…å®¹å¯ç¿»è¯‘"
        
        prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼Œä¿æŒåŸæ„å¹¶ç¡®ä¿ç¿»è¯‘è‡ªç„¶æµç•…ï¼š

åŸæ–‡ï¼š
{text}

ç¿»è¯‘ï¼š"""

        try:
            response = await self.llm.agenerate([[HumanMessage(content=prompt)]])
            translation = response.generations[0][0].text.strip()
            
            logger.info(f"æ–‡æœ¬ç¿»è¯‘å®Œæˆï¼Œç›®æ ‡è¯­è¨€: {target_language}")
            return translation
            
        except Exception as e:
            logger.error(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
            return f"ç¿»è¯‘å¤±è´¥: {str(e)}"
    
    async def extract_key_points(self, chunks: List[DocumentChunk]) -> List[str]:
        """æå–å…³é”®è¦ç‚¹"""
        if not chunks:
            return ["æ— æ–‡æ¡£å†…å®¹å¯åˆ†æ"]
        
        combined_content = "\n\n".join([chunk.content for chunk in chunks[:5]])
        
        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–5-10ä¸ªå…³é”®è¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{combined_content}

å…³é”®è¦ç‚¹ï¼š
1."""

        try:
            response = await self.llm.agenerate([[HumanMessage(content=prompt)]])
            result = response.generations[0][0].text.strip()
            
            # è§£æè¦ç‚¹åˆ—è¡¨
            points = []
            lines = result.split('\n')
            for line in lines:
                line = line.strip()
                if line and (line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.')) or 
                           line.startswith(('â€¢', '-', '*'))):
                    # å»é™¤åºå·
                    point = line.split('.', 1)[-1].strip() if '.' in line else line[1:].strip()
                    if point:
                        points.append(point)
            
            logger.info(f"å…³é”®è¦ç‚¹æå–å®Œæˆï¼Œå…±{len(points)}ä¸ªè¦ç‚¹")
            return points if points else [result]
            
        except Exception as e:
            logger.error(f"å…³é”®è¦ç‚¹æå–å¤±è´¥: {str(e)}")
            return [f"å…³é”®è¦ç‚¹æå–å¤±è´¥: {str(e)}"]
    
    async def analyze_document_structure(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        if not chunks:
            return {"error": "æ— æ–‡æ¡£å†…å®¹å¯åˆ†æ"}
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chunks = len(chunks)
        total_length = sum(len(chunk.content) for chunk in chunks)
        
        # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
        file_types = {}
        for chunk in chunks:
            file_type = chunk.metadata.get("file_type", "unknown")
            if file_type not in file_types:
                file_types[file_type] = []
            file_types[file_type].append(chunk)
        
        # æŒ‰æ¥æºæ–‡ä»¶åˆ†ç»„
        source_files = {}
        for chunk in chunks:
            source = chunk.source_file
            if source not in source_files:
                source_files[source] = []
            source_files[source].append(chunk)
        
        structure = {
            "total_chunks": total_chunks,
            "total_length": total_length,
            "average_chunk_length": total_length // total_chunks if total_chunks > 0 else 0,
            "file_types": {ft: len(chunks) for ft, chunks in file_types.items()},
            "source_files": {sf: len(chunks) for sf, chunks in source_files.items()},
            "has_database_content": any(chunk.metadata.get("source_type") == "database" for chunk in chunks)
        }
        
        logger.info(f"æ–‡æ¡£ç»“æ„åˆ†æå®Œæˆ: {total_chunks}ä¸ªå—ï¼Œ{len(source_files)}ä¸ªæºæ–‡ä»¶")
        return structure
    
    async def generate_questions(self, chunks: List[DocumentChunk], count: int = 5) -> List[str]:
        """åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆç›¸å…³é—®é¢˜"""
        if not chunks:
            return ["æ— æ–‡æ¡£å†…å®¹å¯ç”Ÿæˆé—®é¢˜"]
        
        combined_content = "\n\n".join([chunk.content for chunk in chunks[:8]])
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆ{count}ä¸ªç›¸å…³çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åº”è¯¥ï¼š
1. æ¶µç›–æ–‡æ¡£çš„ä¸»è¦å†…å®¹
2. å…·æœ‰å®é™…æ„ä¹‰å’Œä»·å€¼
3. å¯ä»¥é€šè¿‡æ–‡æ¡£å†…å®¹å›ç­”
4. ä½¿ç”¨ä¸­æ–‡

æ–‡æ¡£å†…å®¹ï¼š
{combined_content}

ç›¸å…³é—®é¢˜ï¼š
1."""

        try:
            response = await self.llm.agenerate([[HumanMessage(content=prompt)]])
            result = response.generations[0][0].text.strip()
            
            # è§£æé—®é¢˜åˆ—è¡¨
            questions = []
            lines = result.split('\n')
            for line in lines:
                line = line.strip()
                if line and any(line.startswith(f'{i}.') for i in range(1, count + 2)):
                    # å»é™¤åºå·
                    question = line.split('.', 1)[-1].strip()
                    if question and question.endswith('?'):
                        questions.append(question)
            
            logger.info(f"ç›¸å…³é—®é¢˜ç”Ÿæˆå®Œæˆï¼Œå…±{len(questions)}ä¸ªé—®é¢˜")
            return questions if questions else [result]
            
        except Exception as e:
            logger.error(f"é—®é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}")
            return [f"é—®é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}"]
    
    async def compare_documents(self, chunks_a: List[DocumentChunk], chunks_b: List[DocumentChunk]) -> str:
        """æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„å¼‚åŒ"""
        if not chunks_a or not chunks_b:
            return "éœ€è¦ä¸¤ä¸ªæ–‡æ¡£æ‰èƒ½è¿›è¡Œæ¯”è¾ƒ"
        
        content_a = "\n".join([chunk.content for chunk in chunks_a[:5]])
        content_b = "\n".join([chunk.content for chunk in chunks_b[:5]])
        
        prompt = f"""è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ–‡æ¡£çš„å¼‚åŒï¼Œåˆ†æå®ƒä»¬çš„ï¼š
1. å…±åŒç‚¹
2. ä¸åŒç‚¹
3. å„è‡ªçš„ç‰¹è‰²
4. æ•´ä½“å…³ç³»

æ–‡æ¡£Aï¼š
{content_a}

æ–‡æ¡£Bï¼š
{content_b}

æ¯”è¾ƒåˆ†æï¼š"""

        try:
            response = await self.llm.agenerate([[HumanMessage(content=prompt)]])
            comparison = response.generations[0][0].text.strip()
            
            logger.info("æ–‡æ¡£æ¯”è¾ƒå®Œæˆ")
            return comparison
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ¯”è¾ƒå¤±è´¥: {str(e)}")
            return f"æ–‡æ¡£æ¯”è¾ƒå¤±è´¥: {str(e)}"
    
    async def extract_entities(self, chunks: List[DocumentChunk]) -> Dict[str, List[str]]:
        """æå–æ–‡æ¡£ä¸­çš„å®ä½“ï¼ˆäººåã€åœ°åã€æœºæ„ç­‰ï¼‰"""
        if not chunks:
            return {"error": ["æ— æ–‡æ¡£å†…å®¹å¯åˆ†æ"]}
        
        combined_content = "\n\n".join([chunk.content for chunk in chunks[:5]])
        
        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–é‡è¦çš„å®ä½“ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
- äººå
- åœ°å
- æœºæ„å
- æ—¥æœŸæ—¶é—´
- æ•°å­—é‡‘é¢
- ä¸“ä¸šæœ¯è¯­

æ–‡æ¡£å†…å®¹ï¼š
{combined_content}

è¯·æŒ‰ç±»åˆ«åˆ—å‡ºæå–çš„å®ä½“ï¼š

äººåï¼š
åœ°åï¼š
æœºæ„åï¼š
æ—¥æœŸæ—¶é—´ï¼š
æ•°å­—é‡‘é¢ï¼š
ä¸“ä¸šæœ¯è¯­ï¼š"""

        try:
            response = await self.llm.agenerate([[HumanMessage(content=prompt)]])
            result = response.generations[0][0].text.strip()
            
            # è§£æå®ä½“
            entities = {
                "äººå": [],
                "åœ°å": [],
                "æœºæ„å": [],
                "æ—¥æœŸæ—¶é—´": [],
                "æ•°å­—é‡‘é¢": [],
                "ä¸“ä¸šæœ¯è¯­": []
            }
            
            current_category = None
            lines = result.split('\n')
            
            for line in lines:
                line = line.strip()
                if line.endswith('ï¼š') or line.endswith(':'):
                    category = line.rstrip('ï¼š:')
                    if category in entities:
                        current_category = category
                elif current_category and line:
                    # ç§»é™¤åºå·å’Œç‰¹æ®Šç¬¦å·
                    entity = line.lstrip('- â€¢ * 1234567890. ').strip()
                    if entity:
                        entities[current_category].append(entity)
            
            logger.info(f"å®ä½“æå–å®Œæˆ: {sum(len(v) for v in entities.values())}ä¸ªå®ä½“")
            return entities
            
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {str(e)}")
            return {"error": [f"å®ä½“æå–å¤±è´¥: {str(e)}"]}

    async def process_and_store_documents(self, state: AgentState) -> AgentState:
        """å¤„ç†å¹¶å­˜å‚¨æ–‡æ¡£ï¼Œç„¶åæ›´æ–°è‡ªèº«çš„å‘é‡å­˜å‚¨å¼•ç”¨"""
        processed_state = await self.document_processor.process_documents(state)
        # åœ¨å¤„ç†å®Œæ–‡æ¡£åï¼Œä» document_processor è·å–å‘é‡å­˜å‚¨
        self.vector_store = self.document_processor.vector_store
        logger.info("å‘é‡å­˜å‚¨å·²åœ¨ DocumentTools ä¸­æ›´æ–°ã€‚")
        return processed_state

    async def search_relevant_info(self, state: AgentState) -> AgentState:
        """
        æ ¹æ®ç”¨æˆ·é—®é¢˜ä»å‘é‡å­˜å‚¨å’Œæ•°æ®åº“ä¸­æ£€ç´¢ä¿¡æ¯ã€‚
        """
        logger.info(f"å¼€å§‹æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼ŒæŸ¥è¯¢: '{state.user_query}'")
        logger.info(f"æ£€ç´¢å‚æ•° - top_k: {state.retrieval_top_k}")
        
        # ç¡®ä¿å‘é‡å­˜å‚¨å·²å‡†å¤‡å°±ç»ª
        if not self.vector_store:
            logger.warning("å‘é‡å­˜å‚¨å°šæœªå°±ç»ªï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ã€‚å¯èƒ½éœ€è¦å…ˆè¿è¡Œæ–‡æ¡£å¤„ç†ã€‚")
            state.retrieved_info = "é”™è¯¯ï¼šå‘é‡å­˜å‚¨æœªå°±ç»ªã€‚"
            return state

        logger.info(f"å‘é‡å­˜å‚¨å·²å°±ç»ªï¼Œæ–‡æ¡£å—æ€»æ•°: {len(state.doc_chunks)}")

        # 1. å‘é‡å­˜å‚¨ç›¸ä¼¼æ€§æœç´¢
        logger.info("å¼€å§‹å‘é‡ç›¸ä¼¼æ€§æœç´¢...")
        similar_results = self.document_processor.search_similar_chunks(
            state.user_query, 
            top_k=state.retrieval_top_k
        )
        
        logger.info(f"å‘é‡æœç´¢è¿”å› {len(similar_results)} ä¸ªç»“æœ")
        
        # 2. æ•°æ®åº“ä¿¡æ¯æ£€ç´¢ (è¿™é‡Œå¯ä»¥æ·»åŠ æ ¹æ® state.database_schema çš„æ£€ç´¢é€»è¾‘)
        # ... (æš‚æœªå®ç°)

        # 3. ç»„åˆæ£€ç´¢åˆ°çš„ä¿¡æ¯
        context = "ä»æ–‡æ¡£ä¸­æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼š\n"
        relevant_chunks = []
        
        if similar_results:
            logger.info("å¤„ç†å‘é‡æœç´¢ç»“æœ...")
            # æ ¹æ®ç´¢å¼•è·å–æ–‡æ¡£å—å†…å®¹
            all_chunks = state.doc_chunks
            for idx, score in similar_results:
                if idx < len(all_chunks):
                    chunk = all_chunks[idx]
                    relevant_chunks.append(chunk)
                    
                    # è¯¦ç»†åˆ†ææ¯ä¸ªæ£€ç´¢ç»“æœ
                    chunk_type = chunk.metadata.get('file_type', 'unknown')
                    logger.info(f"ç»“æœ {len(relevant_chunks)}: ç´¢å¼•={idx}, ç›¸ä¼¼åº¦={score:.4f}")
                    logger.info(f"  æ¥æºæ–‡ä»¶: {chunk.source_file}")
                    logger.info(f"  æ–‡ä»¶ç±»å‹: {chunk_type}")
                    logger.info(f"  å†…å®¹é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
                    logger.debug(f"  å†…å®¹é¢„è§ˆ: {chunk.content[:100]}...")
                    
                    # ç‰¹åˆ«æ£€æŸ¥PDFå†…å®¹
                    if chunk_type == 'pdf':
                        logger.info(f"  ğŸ“„ è¿™æ˜¯PDFå†…å®¹å—")
                        if 'longliang' in chunk.content.lower() or 'long liang' in chunk.content.lower():
                            logger.info(f"  âœ… å‘ç°åŒ…å« LongLiang çš„å†…å®¹ï¼")
                        else:
                            logger.debug(f"  âŒ ä¸åŒ…å« LongLiang")
                    elif chunk_type == 'csv':
                        logger.debug(f"  ğŸ“Š è¿™æ˜¯CSVæ•°æ®å—")
                    else:
                        logger.debug(f"  ğŸ“ æ–‡ä»¶ç±»å‹: {chunk_type}")
                    
                    context += f"- æ¥æº: {chunk.source_file} ({chunk_type})\n  å†…å®¹: {chunk.content}\n\n"
                else:
                    logger.warning(f"ç´¢å¼• {idx} è¶…å‡ºæ–‡æ¡£å—èŒƒå›´ (æ€»æ•°: {len(all_chunks)})")
        else:
            logger.warning("å‘é‡æœç´¢æ²¡æœ‰è¿”å›ä»»ä½•ç»“æœ")
            context += "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚\n"

        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†ç›¸å…³å†…å®¹
        found_relevant = any(
            "longliang" in chunk.content.lower() or "long liang" in chunk.content.lower()
            for chunk in relevant_chunks
        )
        
        # å¦‚æœå‘é‡æœç´¢æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå°è¯•å…³é”®è¯æœç´¢
        if not found_relevant and ("longliang" in state.user_query.lower() or "long liang" in state.user_query.lower()):
            logger.info("å‘é‡æœç´¢æœªæ‰¾åˆ°LongLiangç›¸å…³å†…å®¹ï¼Œå¯åŠ¨å…³é”®è¯æœç´¢...")
            
            # å…³é”®è¯æœç´¢
            keyword_chunks = []
            search_terms = ["longliang", "long liang", "liang"]
            
            for chunk in state.doc_chunks:
                content_lower = chunk.content.lower()
                if any(term in content_lower for term in search_terms):
                    keyword_chunks.append(chunk)
                    logger.info(f"âœ… å…³é”®è¯æœç´¢æ‰¾åˆ°åŒ¹é…: {chunk.source_file}")
                    logger.debug(f"   å†…å®¹é¢„è§ˆ: {chunk.content[:200]}...")
            
            if keyword_chunks:
                logger.info(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(keyword_chunks)} ä¸ªç›¸å…³æ–‡æ¡£å—")
                # ä½¿ç”¨å…³é”®è¯æœç´¢ç»“æœæ›¿æ¢æˆ–è¡¥å……å‘é‡æœç´¢ç»“æœ
                relevant_chunks = keyword_chunks[:state.retrieval_top_k]
            else:
                logger.warning("å…³é”®è¯æœç´¢ä¹Ÿæœªæ‰¾åˆ°LongLiangç›¸å…³å†…å®¹")
        
        logger.info(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³æ–‡æ¡£å—")
        
        # æ›´æ–°çŠ¶æ€
        state.retrieved_info = context
        if relevant_chunks:
            from ..schemas.agent_state import RetrievalResult
            state.retrieval_results = RetrievalResult(
                chunks=relevant_chunks[:state.retrieval_top_k],
                scores=[1.0] * len(relevant_chunks[:state.retrieval_top_k]),
                query=state.user_query
            )
            state.relevant_context = context
        
        return state


# å·¥å…·å‡½æ•°
async def batch_process_documents(documents: List[List[DocumentChunk]], tool_function, **kwargs) -> List[Any]:
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£"""
    tools = DocumentTools()
    
    tasks = []
    for doc_chunks in documents:
        task = getattr(tools, tool_function)(doc_chunks, **kwargs)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results


def create_document_tools(ollama_provider: OllamaProvider) -> DocumentTools:
    """åˆ›å»ºæ–‡æ¡£å·¥å…·å®ä¾‹"""
    return DocumentTools(ollama_provider) 