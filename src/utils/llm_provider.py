"""
LLMæä¾›å•†ç®¡ç†å™¨
æ”¯æŒOllamaæœ¬åœ°æ¨¡å‹å’ŒOpenAIäº‘ç«¯æ¨¡å‹çš„ç»Ÿä¸€æ¥å£
"""
import asyncio
import httpx
from typing import Dict, Any, Optional, AsyncGenerator, List
from abc import ABC, abstractmethod

from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain.callbacks import get_openai_callback
from loguru import logger

try:
    from langchain_community.llms import Ollama
    from langchain_community.chat_models import ChatOllama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("Ollama ç›¸å…³åŒ…æœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹")


class BaseLLMProvider(ABC):
    """LLMæä¾›å•†åŸºç±»"""
    
    @abstractmethod
    async def generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """ç”Ÿæˆå›ç­”"""
        pass
    
    @abstractmethod
    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass


class OllamaProvider(BaseLLMProvider):
    """Ollamaæœ¬åœ°æ¨¡å‹æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        if not OLLAMA_AVAILABLE:
            raise ImportError("è¯·å®‰è£… langchain-community ä»¥ä½¿ç”¨ Ollama æ¨¡å‹")
        
        self.config = config
        self.base_url = config.get("ollama_base_url", "http://localhost:11434")
        self.model_name = config.get("llm_model", "qwen2.5:7b")
        self.timeout = config.get("ollama_timeout", 300)
        
        # åˆå§‹åŒ–OllamaèŠå¤©æ¨¡å‹
        self.chat_model = ChatOllama(
            model=self.model_name,
            base_url=self.base_url,
            temperature=config.get("temperature", 0.7),
            num_predict=config.get("max_tokens", 2048),
            top_p=config.get("top_p", 0.9),
            top_k=config.get("top_k", 40),
            keep_alive=config.get("ollama_keep_alive", "5m"),
            timeout=self.timeout,
        )
        
        logger.info(f"åˆå§‹åŒ– Ollama æ¨¡å‹: {self.model_name} @ {self.base_url}")
    
    async def generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """ç”Ÿæˆå›ç­”"""
        try:
            # Ollama æ”¯æŒå¼‚æ­¥è°ƒç”¨
            response = await self.chat_model.agenerate([messages])
            result = response.generations[0][0].text.strip()
            
            logger.info(f"Ollama ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"Ollama ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        try:
            async for chunk in self.chat_model.astream(messages):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"Ollama æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def health_check(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                # æ£€æŸ¥æœåŠ¡çŠ¶æ€
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code != 200:
                    return False
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                if self.model_name not in available_models:
                    logger.warning(f"æ¨¡å‹ {self.model_name} ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­: {available_models}")
                    return False
                
                logger.info(f"Ollama å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œå¯ç”¨æ¨¡å‹: {len(available_models)}")
                return True
                
        except Exception as e:
            logger.error(f"Ollama å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    async def pull_model(self, model_name: Optional[str] = None) -> bool:
        """æ‹‰å–æ¨¡å‹"""
        target_model = model_name or self.model_name
        
        try:
            async with httpx.AsyncClient(timeout=600) as client:  # æ‹‰å–æ¨¡å‹å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´
                response = await client.post(
                    f"{self.base_url}/api/pull",
                    json={"name": target_model}
                )
                
                if response.status_code == 200:
                    logger.info(f"æ¨¡å‹ {target_model} æ‹‰å–æˆåŠŸ")
                    return True
                else:
                    logger.error(f"æ¨¡å‹ {target_model} æ‹‰å–å¤±è´¥: {response.text}")
                    return False
                    
        except Exception as e:
            logger.error(f"æ‹‰å–æ¨¡å‹å¼‚å¸¸: {str(e)}")
            return False


class OpenAIProvider(BaseLLMProvider):
    """OpenAIäº‘ç«¯æ¨¡å‹æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model_name = config.get("llm_model", "gpt-3.5-turbo")
        
        # åˆå§‹åŒ–OpenAIèŠå¤©æ¨¡å‹
        self.chat_model = ChatOpenAI(
            model_name=self.model_name,
            temperature=config.get("temperature", 0.7),
            max_tokens=config.get("max_tokens", 2048),
            streaming=True,
            api_key=config.get("openai_api_key"),
            base_url=config.get("openai_base_url")
        )
        
        logger.info(f"åˆå§‹åŒ– OpenAI æ¨¡å‹: {self.model_name}")
    
    async def generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """ç”Ÿæˆå›ç­”"""
        try:
            with get_openai_callback() as cb:
                response = await self.chat_model.agenerate([messages])
                
                logger.info(f"OpenAI è°ƒç”¨ç»Ÿè®¡ - Tokens: {cb.total_tokens}, æˆæœ¬: ${cb.total_cost:.4f}")
            
            result = response.generations[0][0].text.strip()
            return result
            
        except Exception as e:
            logger.error(f"OpenAI ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        try:
            async for chunk in self.chat_model.astream(messages):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"OpenAI æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def health_check(self) -> bool:
        """æ£€æŸ¥OpenAI APIå¥åº·çŠ¶æ€"""
        try:
            # å°è¯•ä¸€ä¸ªç®€å•çš„APIè°ƒç”¨
            test_messages = [HumanMessage(content="Hello")]
            response = await self.generate(test_messages)
            
            if "å¤±è´¥" in response:
                return False
            
            logger.info("OpenAI API å¥åº·æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"OpenAI å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False


class LLMProviderManager:
    """LLMæä¾›å•†ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.provider_type = config.get("provider", "ollama")
        self.provider: Optional[BaseLLMProvider] = None
        
        # åˆå§‹åŒ–æä¾›å•†
        self._initialize_provider()
    
    def _initialize_provider(self):
        """åˆå§‹åŒ–LLMæä¾›å•†"""
        try:
            if self.provider_type == "ollama":
                self.provider = OllamaProvider(self.config)
            elif self.provider_type == "openai":
                self.provider = OpenAIProvider(self.config)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: {self.provider_type}")
            
            logger.info(f"LLMæä¾›å•†åˆå§‹åŒ–æˆåŠŸ: {self.provider_type}")
            
        except Exception as e:
            logger.error(f"LLMæä¾›å•†åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # å›é€€åˆ°OpenAIï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            if self.provider_type != "openai" and self.config.get("openai_api_key"):
                logger.info("å°è¯•å›é€€åˆ° OpenAI æä¾›å•†")
                self.provider_type = "openai"
                self.provider = OpenAIProvider(self.config)
    
    async def generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """ç”Ÿæˆå›ç­”"""
        if not self.provider:
            return "LLMæä¾›å•†æœªåˆå§‹åŒ–"
        
        return await self.provider.generate(messages, **kwargs)
    
    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        if not self.provider:
            yield "LLMæä¾›å•†æœªåˆå§‹åŒ–"
            return
        
        async for chunk in self.provider.astream(messages, **kwargs):
            yield chunk
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        if not self.provider:
            return False
        
        return await self.provider.health_check()
    
    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–æä¾›å•†ä¿¡æ¯"""
        return {
            "type": self.provider_type,
            "model": self.config.get("llm_model"),
            "base_url": self.config.get(f"{self.provider_type}_base_url"),
            "available": self.provider is not None
        }
    
    async def switch_provider(self, provider_type: str, config_updates: Dict[str, Any] = None):
        """åˆ‡æ¢æä¾›å•†"""
        if config_updates:
            self.config.update(config_updates)
        
        self.provider_type = provider_type
        self.config["provider"] = provider_type
        
        self._initialize_provider()
        
        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        if await self.health_check():
            logger.info(f"æˆåŠŸåˆ‡æ¢åˆ° {provider_type} æä¾›å•†")
        else:
            logger.warning(f"åˆ‡æ¢åˆ° {provider_type} æä¾›å•†åå¥åº·æ£€æŸ¥å¤±è´¥")


def create_llm_provider(config: Dict[str, Any]) -> LLMProviderManager:
    """åˆ›å»ºLLMæä¾›å•†ç®¡ç†å™¨"""
    return LLMProviderManager(config)


# ============== ä¾¿æ·å‡½æ•° ==============

async def test_all_providers(config: Dict[str, Any]):
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æä¾›å•†"""
    results = {}
    
    # æµ‹è¯•Ollama
    if OLLAMA_AVAILABLE:
        try:
            ollama_config = config.copy()
            ollama_config["provider"] = "ollama"
            ollama_manager = create_llm_provider(ollama_config)
            
            health = await ollama_manager.health_check()
            results["ollama"] = {
                "available": health,
                "model": config.get("llm_model", "qwen2.5:7b"),
                "base_url": config.get("ollama_base_url", "http://localhost:11434")
            }
            
        except Exception as e:
            results["ollama"] = {"available": False, "error": str(e)}
    
    # æµ‹è¯•OpenAI
    if config.get("openai_api_key"):
        try:
            openai_config = config.copy()
            openai_config["provider"] = "openai"
            openai_manager = create_llm_provider(openai_config)
            
            health = await openai_manager.health_check()
            results["openai"] = {
                "available": health,
                "model": config.get("llm_model", "gpt-3.5-turbo")
            }
            
        except Exception as e:
            results["openai"] = {"available": False, "error": str(e)}
    
    return results


if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    import asyncio
    from config import get_config
    
    async def main():
        config = get_config("standard")
        
        print("ğŸ§ª æµ‹è¯•æ‰€æœ‰LLMæä¾›å•†...")
        results = await test_all_providers(config)
        
        for provider, info in results.items():
            status = "âœ…" if info.get("available") else "âŒ"
            print(f"{status} {provider.upper()}: {info}")
        
        # æµ‹è¯•ç”Ÿæˆ
        if any(info.get("available") for info in results.values()):
            manager = create_llm_provider(config)
            test_message = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")]
            
            print(f"\nğŸ¤– æµ‹è¯•ç”ŸæˆåŠŸèƒ½...")
            response = await manager.generate(test_message)
            print(f"å›ç­”: {response}")
    
    asyncio.run(main()) 