"""
Ollamaæœ¬åœ°æ¨¡å‹æä¾›å•†
æ”¯æŒOllamaæœ¬åœ°æ¨¡å‹çš„ç»Ÿä¸€æ¥å£
"""
import asyncio
import httpx
from typing import Dict, Any, Optional, AsyncGenerator, List

from langchain.schema import BaseMessage, HumanMessage
from loguru import logger

try:
    # ä¼˜å…ˆä½¿ç”¨æ–°çš„ langchain-ollama åŒ…
    from langchain_ollama import ChatOllama
    OLLAMA_AVAILABLE = True
except ImportError:
    try:
        # å›é€€åˆ°æ—§çš„åŒ…
        from langchain_community.chat_models import ChatOllama
        OLLAMA_AVAILABLE = True
    except ImportError:
        OLLAMA_AVAILABLE = False
        logger.warning("Ollama ç›¸å…³åŒ…æœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹")


class OllamaProvider:
    """Ollamaæœ¬åœ°æ¨¡å‹æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        if not OLLAMA_AVAILABLE:
            raise ImportError("è¯·å®‰è£… langchain-community ä»¥ä½¿ç”¨ Ollama æ¨¡å‹")
        
        self.config = config
        self.base_url = config.get("ollama_base_url", "http://localhost:11434")
        self.model_name = config.get("llm_model", "qwen2.5:7b")
        self.timeout = config.get("ollama_timeout", 300)
        
        # åˆå§‹åŒ–OllamaèŠå¤©æ¨¡å‹
        self.chat_model = ChatOllama(
            model=self.model_name,
            base_url=self.base_url,
            temperature=config.get("temperature", 0.7),
            num_predict=config.get("max_tokens", 2048),
            top_p=config.get("top_p", 0.9),
            top_k=config.get("top_k", 40),
            keep_alive=config.get("ollama_keep_alive", "5m"),
            timeout=self.timeout,
        )
        
        logger.info(f"åˆå§‹åŒ– Ollama æ¨¡å‹: {self.model_name} @ {self.base_url}")
    
    def get_llm(self) -> ChatOllama:
        """è¿”å›åˆå§‹åŒ–çš„ChatOllamaå®ä¾‹"""
        return self.chat_model

    async def generate(self, messages: List[BaseMessage], **kwargs) -> str:
        """ç”Ÿæˆå›ç­”"""
        try:
            # Ollama æ”¯æŒå¼‚æ­¥è°ƒç”¨
            response = await self.chat_model.agenerate([messages])
            result = response.generations[0][0].text.strip()
            
            logger.info(f"Ollama ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"Ollama ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def astream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        try:
            async for chunk in self.chat_model.astream(messages):
                if chunk.content:
                    yield chunk.content
        except Exception as e:
            logger.error(f"Ollama æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def health_check(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            async with httpx.AsyncClient(timeout=10) as client:
                # æ£€æŸ¥æœåŠ¡çŠ¶æ€
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code != 200:
                    return False
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                if self.model_name not in available_models:
                    logger.warning(f"æ¨¡å‹ {self.model_name} ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­: {available_models}")
                    return False
                
                logger.info(f"Ollama å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œå¯ç”¨æ¨¡å‹: {len(available_models)}")
                return True
                
        except Exception as e:
            logger.error(f"Ollama å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    async def pull_model(self, model_name: Optional[str] = None) -> bool:
        """æ‹‰å–æ¨¡å‹"""
        target_model = model_name or self.model_name
        
        try:
            async with httpx.AsyncClient(timeout=600) as client:  # æ‹‰å–æ¨¡å‹å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´
                response = await client.post(
                    f"{self.base_url}/api/pull",
                    json={"name": target_model}
                )
                
                if response.status_code == 200:
                    logger.info(f"æ¨¡å‹ {target_model} æ‹‰å–æˆåŠŸ")
                    return True
                else:
                    logger.error(f"æ¨¡å‹ {target_model} æ‹‰å–å¤±è´¥: {response.text}")
                    return False
                    
        except Exception as e:
            logger.error(f"æ‹‰å–æ¨¡å‹å¼‚å¸¸: {str(e)}")
            return False


def create_ollama_provider(config: Dict[str, Any]) -> OllamaProvider:
    """åˆ›å»ºOllamaæä¾›å•†"""
    return OllamaProvider(config)


async def test_ollama_connection(config: Dict[str, Any]) -> Dict[str, Any]:
    """æµ‹è¯•Ollamaè¿æ¥"""
    result = {
        "connected": False,
        "models": [],
        "error": None
    }
    
    try:
        base_url = config.get("ollama_base_url", "http://localhost:11434")
        
        async with httpx.AsyncClient(timeout=10) as client:
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            response = await client.get(f"{base_url}/api/tags")
            
            if response.status_code == 200:
                data = response.json()
                result["connected"] = True
                result["models"] = [model["name"] for model in data.get("models", [])]
            else:
                result["error"] = f"HTTP {response.status_code}: {response.text}"
                
    except Exception as e:
        result["error"] = str(e)
    
    return result


if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    import asyncio
    
    async def main():
        from config import get_config
        
        config = get_config("standard")
        
        print("ğŸ§ª æµ‹è¯•Ollamaè¿æ¥...")
        test_result = await test_ollama_connection(config)
        
        if test_result["connected"]:
            print(f"âœ… Ollamaè¿æ¥æˆåŠŸ")
            print(f"ğŸ“¦ å¯ç”¨æ¨¡å‹: {test_result['models']}")
            
            # æµ‹è¯•æ¨¡å‹
            if config.get("llm_model") in test_result["models"]:
                provider = create_ollama_provider(config)
                test_message = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")]
                
                print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹ç”Ÿæˆ...")
                response = await provider.generate(test_message)
                print(f"å›ç­”: {response}")
            else:
                print(f"âš ï¸ æ‰€éœ€æ¨¡å‹ {config.get('llm_model')} æœªå®‰è£…")
        else:
            print(f"âŒ Ollamaè¿æ¥å¤±è´¥: {test_result['error']}")
    
    asyncio.run(main()) 